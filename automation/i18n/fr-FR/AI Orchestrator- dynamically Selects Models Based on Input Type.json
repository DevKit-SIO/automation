{
  "id": 7004,
  "name": "Orchestrateur IA : sélectionne dynamiquement des modèles en fonction du type d'entrée",
  "views": 6576,
  "recentViews": 43,
  "totalViews": 6576,
  "createdAt": "2025-08-05T10:21:49.772Z",
  "description": "Ce flux de travail est conçu pour **acheminer intelligemment les requêtes des utilisateurs vers le modèle de langage large (LLM) le plus adapté** en fonction du type de demande reçue dans un environnement de chat. Il utilise une classification structurée et une sélection de modèles pour optimiser à la fois la performance et l'efficacité des coûts dans les conversations alimentées par l'IA.\n\nIl achemine dynamiquement les demandes vers des modèles d'IA spécialisés en fonction du type de contenu, optimisant ainsi la qualité et l'efficacité des réponses.\n\n---\n\n###  **Avantages**\n\n* **Acheminement intelligent des modèles** : Réduit les coûts en utilisant des modèles plus légers pour des tâches générales et en réservant des modèles plus lourds pour des besoins complexes.\n* **Scalabilité** : Facilement extensible en ajoutant plus de types de demandes ou de LLM.\n* **Maintenabilité** : Séparation claire de la logique entre la classification, l'acheminement des modèles et l'exécution.\n* **Personnalisation** : Peut être intégré avec des ID de session pour une mémoire par utilisateur, permettant des conversations personnalisées.\n* **Optimisation de la vitesse** : Des modèles rapides comme `GPT-4.1 mini` ou `Gemini Flash` sont choisis pour les tâches où la vitesse est une priorité.\n---\n\n### **Comment ça fonctionne**\n1. **Gestion des entrées** :  \n   - Le flux de travail commence avec le nœud \"Lorsque le message de chat est reçu\", qui déclenche le processus lorsqu'un message de chat est reçu. L'entrée comprend le message de chat (`chatInput`) et un ID de session (`sessionId`).\n\n2. **Classification des demandes** :  \n   - Le nœud \"Type de demande\" utilise un modèle OpenAI (`gpt-4.1-mini`) pour classer la demande entrante en l'une des quatre catégories :  \n     - `général` : Pour les requêtes générales.  \n     - `raisonnement` : Pour les questions basées sur le raisonnement.  \n     - `codage` : Pour les demandes liées au code.  \n     - `recherche` : Pour les requêtes nécessitant des outils de recherche.  \n   - La classification est structurée à l'aide du nœud \"Analyseur de sortie structuré\", qui impose un format de sortie cohérent.\n\n3. **Sélection de modèle** :  \n   - Le nœud \"Sélecteur de modèle\" achemine la demande vers l'un des quatre modèles d'IA en fonction de la classification :  \n     - **Opus 4** (Claude 4 Sonnet) : Utilisé pour les demandes de `codage`.  \n     - **Gemini Thinking Pro** : Utilisé pour les demandes de `raisonnement`.  \n     - **GPT 4.1 mini** : Utilisé pour les demandes `générales`.  \n     - **Perplexity** : Utilisé pour les demandes de `recherche` (liées à Google).  \n\n4. **Traitement par l'IA** :  \n   - Le modèle sélectionné traite la demande via le nœud \"Agent IA\", qui comprend des étapes intermédiaires pour des tâches complexes.  \n   - Le nœud \"Mémoire simple\" conserve le contexte de la session en utilisant le `sessionId` fourni, permettant des conversations multi-tours.  \n\n5. **Sortie** :  \n   - La réponse finale est générée par le modèle choisi et renvoyée à l'utilisateur.  \n\n---\n\n### **Étapes de configuration**\n1. **Configurer le déclencheur** :  \n   - Assurez-vous que le nœud \"Lorsque le message de chat est reçu\" est configuré avec l'ID de webhook correct pour recevoir les entrées de chat.  \n\n2. **Définir la logique de classification** :  \n   - Ajustez l'invite dans le nœud \"Type de demande\" pour affiner la précision de la classification.  \n   - Vérifiez que le schéma de sortie dans le nœud \"Analyseur de sortie structuré\" correspond aux catégories attendues (`général`, `raisonnement`, `codage`, `recherche`).  \n\n3. **Connecter les modèles d'IA** :  \n   - Liez chaque nœud de modèle (Opus 4, Gemini Thinking Pro, GPT 4.1 mini, Perplexity) au nœud \"Sélecteur de modèle\".  \n   - Assurez-vous que les identifiants (clés API) pour chaque modèle sont correctement configurés dans leurs nœuds respectifs.  \n\n4. **Configurer la mémoire** :  \n   - Configurez le nœud \"Mémoire simple\" pour utiliser le `sessionId` de l'entrée pour la rétention de contexte.  \n\n5. **Tester le flux de travail** :  \n   - Envoyez des entrées de test pour vérifier la classification et l'acheminement des modèles.  \n   - Vérifiez les sorties intermédiaires (par exemple, `request_type`) pour garantir la sélection correcte du modèle.  \n\n6. **Activer le flux de travail** :  \n   - Basculez le flux de travail sur \"Actif\" dans n8n après les tests.  \n\n---\n### **Besoin d'aide pour la personnalisation ?**  \n[Contactez-moi](mailto:info@n3w.it) pour des conseils et un support ou ajoutez-moi sur [Linkedin](https://www.linkedin.com/in/davideboizza/).",
  "workflow": {
    "id": "abSdfsaYgkssXX7g",
    "meta": {
      "instanceId": "a4bfc93e975ca233ac45ed7c9227d84cf5a2329310525917adaf3312e10d5462",
      "templateCredsSetupCompleted": true
    },
    "name": "Dynamically Selects Models Based on Input Type",
    "tags": [],
    "nodes": [
      {
        "id": "daf34daa-19e5-42a8-b820-5aa3d78c29a4",
        "name": "When chat message received",
        "type": "@n8n/n8n-nodes-langchain.chatTrigger",
        "position": [
          -528,
          -112
        ],
        "webhookId": "56b65a7f-0698-4e99-81eb-fd87e0cb5bfa",
        "parameters": {
          "options": {}
        },
        "typeVersion": 1.1
      },
      {
        "id": "85be9290-50ac-457e-9fa1-0c00a88667da",
        "name": "AI Agent",
        "type": "@n8n/n8n-nodes-langchain.agent",
        "position": [
          160,
          -112
        ],
        "parameters": {
          "text": "={{ $('When chat message received').item.json.chatInput }}",
          "options": {
            "returnIntermediateSteps": true
          },
          "promptType": "define"
        },
        "typeVersion": 2.1
      },
      {
        "id": "0721d812-2dc6-4069-87e6-844b8f94214b",
        "name": "Model Selector",
        "type": "@n8n/n8n-nodes-langchain.modelSelector",
        "position": [
          80,
          128
        ],
        "parameters": {
          "rules": {
            "rule": [
              {
                "conditions": {
                  "options": {
                    "version": 2,
                    "leftValue": "",
                    "caseSensitive": true,
                    "typeValidation": "strict"
                  },
                  "combinator": "and",
                  "conditions": [
                    {
                      "id": "976d83bb-7e9e-4aab-9722-25a9e238164f",
                      "operator": {
                        "name": "filter.operator.equals",
                        "type": "string",
                        "operation": "equals"
                      },
                      "leftValue": "={{ $json.output.request_type }}",
                      "rightValue": "coding"
                    }
                  ]
                }
              },
              {
                "conditions": {
                  "options": {
                    "version": 2,
                    "leftValue": "",
                    "caseSensitive": true,
                    "typeValidation": "strict"
                  },
                  "combinator": "and",
                  "conditions": [
                    {
                      "id": "1e68688d-73fe-47c1-9b35-a1e226220bcd",
                      "operator": {
                        "name": "filter.operator.equals",
                        "type": "string",
                        "operation": "equals"
                      },
                      "leftValue": "={{ $json.output.request_type }}",
                      "rightValue": "reasoning"
                    }
                  ]
                },
                "modelIndex": 2
              },
              {
                "conditions": {
                  "options": {
                    "version": 2,
                    "leftValue": "",
                    "caseSensitive": true,
                    "typeValidation": "strict"
                  },
                  "combinator": "and",
                  "conditions": [
                    {
                      "id": "61d58197-db59-4cd7-bc41-bbeaf5e7b069",
                      "operator": {
                        "name": "filter.operator.equals",
                        "type": "string",
                        "operation": "equals"
                      },
                      "leftValue": "={{ $json.output.request_type }}",
                      "rightValue": "general"
                    }
                  ]
                },
                "modelIndex": 3
              },
              {
                "conditions": {
                  "options": {
                    "version": 2,
                    "leftValue": "",
                    "caseSensitive": true,
                    "typeValidation": "strict"
                  },
                  "combinator": "and",
                  "conditions": [
                    {
                      "id": "fca2ec99-fd1d-458f-9919-73bfbba55c4f",
                      "operator": {
                        "name": "filter.operator.equals",
                        "type": "string",
                        "operation": "equals"
                      },
                      "leftValue": "={{ $json.output.request_type }}",
                      "rightValue": "search"
                    }
                  ]
                },
                "modelIndex": 4
              }
            ]
          },
          "numberInputs": 4
        },
        "typeVersion": 1
      },
      {
        "id": "449b0bae-3749-493d-b6f6-dad155537bc9",
        "name": "Structured Output Parser",
        "type": "@n8n/n8n-nodes-langchain.outputParserStructured",
        "position": [
          -80,
          32
        ],
        "parameters": {
          "schemaType": "manual",
          "inputSchema": "{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"request_type\": {\n\t\t\t\"type\": \"string\"\n\t\t}\n\t}\n}"
        },
        "typeVersion": 1.3
      },
      {
        "id": "03339701-3ed8-43c9-a490-3ebca30d39bb",
        "name": "Simple Memory",
        "type": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
        "position": [
          400,
          128
        ],
        "parameters": {
          "sessionKey": "={{ $('When chat message received').item.json.sessionId }}",
          "sessionIdType": "customKey"
        },
        "typeVersion": 1.3
      },
      {
        "id": "1d86d306-bbdd-45b0-9b96-fce0cbcdc0a0",
        "name": "Request Type",
        "type": "@n8n/n8n-nodes-langchain.chainLlm",
        "position": [
          -288,
          -112
        ],
        "parameters": {
          "batching": {},
          "messages": {
            "messageValues": [
              {
                "message": "=Your task is to classify the type of request you receive as input.\nYou must provide the following output:\n- general: if it is a general request\n- reasoning: if it is a reasoning request\n- coding: if it is a request related to code development\n- search: if it is a request that involves the use of Google tools"
              }
            ]
          },
          "hasOutputParser": true
        },
        "typeVersion": 1.7
      },
      {
        "id": "f49883cd-fc48-4c26-8596-6267beb74c3d",
        "name": "Opus 4",
        "type": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
        "position": [
          -64,
          352
        ],
        "parameters": {
          "model": {
            "__rl": true,
            "mode": "list",
            "value": "claude-sonnet-4-20250514",
            "cachedResultName": "Claude 4 Sonnet"
          },
          "options": {}
        },
        "credentials": {
          "anthropicApi": {
            "id": "NNTZAD0Gmf7lcniq",
            "name": "Anthropic account"
          }
        },
        "typeVersion": 1.3
      },
      {
        "id": "0f19c20b-298b-45d8-b855-3a92c0dac675",
        "name": "Gemini Thinking Pro",
        "type": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
        "position": [
          80,
          352
        ],
        "parameters": {
          "options": {},
          "modelName": "models/gemini-2.0-flash-thinking-exp"
        },
        "credentials": {
          "googlePalmApi": {
            "id": "AaNPKXAphyMzRgfA",
            "name": "Google Gemini(PaLM) (Eure)"
          }
        },
        "typeVersion": 1
      },
      {
        "id": "d2676326-64f2-47db-84f4-17fbf194d31b",
        "name": "GPT 4.1 mini",
        "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
        "position": [
          224,
          352
        ],
        "parameters": {
          "model": {
            "__rl": true,
            "mode": "list",
            "value": "gpt-4.1-mini"
          },
          "options": {}
        },
        "credentials": {
          "openAiApi": {
            "id": "TefveNaDaMERl1hY",
            "name": "OpenAi account (Eure)"
          }
        },
        "typeVersion": 1.2
      },
      {
        "id": "1f8d38d9-0298-4588-a763-7fac0132edf5",
        "name": "Perplexity",
        "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
        "position": [
          352,
          352
        ],
        "parameters": {
          "model": "perplexity/sonar",
          "options": {}
        },
        "credentials": {
          "openRouterApi": {
            "id": "pb06rfB4xmxzVe3Q",
            "name": "OpenRouter"
          }
        },
        "typeVersion": 1
      },
      {
        "id": "fc7a67be-d46a-4fec-b1d2-9f8ce3b86462",
        "name": "OpenAI Chat Model",
        "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
        "position": [
          -320,
          48
        ],
        "parameters": {
          "model": {
            "__rl": true,
            "mode": "list",
            "value": "gpt-4.1-mini"
          },
          "options": {}
        },
        "credentials": {
          "openAiApi": {
            "id": "TefveNaDaMERl1hY",
            "name": "OpenAi account (Eure)"
          }
        },
        "typeVersion": 1.2
      },
      {
        "id": "de2fc53a-bf97-475a-b978-0cde88483ee0",
        "name": "Sticky Note",
        "type": "n8n-nodes-base.stickyNote",
        "position": [
          -528,
          -416
        ],
        "parameters": {
          "width": 624,
          "height": 256,
          "content": "## AI Orchestrator: dynamically Selects Models Based on Input Type\n\nThis workflow is designed to intelligently **route user queries to the most suitable large language model (LLM)** based on the type of request received in a chat environment. It uses structured classification and model selection to optimize both performance and cost-efficiency in AI-driven conversations.\n\nIt dynamically routes requests to specialized AI models based on content type, optimizing response quality and efficiency."
        },
        "typeVersion": 1
      }
    ],
    "active": false,
    "pinData": {},
    "settings": {
      "executionOrder": "v1"
    },
    "versionId": "d852ebdf-2860-4611-a58e-e58c8cd4cc35",
    "connections": {
      "Opus 4": {
        "ai_languageModel": [
          [
            {
              "node": "Model Selector",
              "type": "ai_languageModel",
              "index": 0
            }
          ]
        ]
      },
      "Perplexity": {
        "ai_languageModel": [
          [
            {
              "node": "Model Selector",
              "type": "ai_languageModel",
              "index": 3
            }
          ]
        ]
      },
      "GPT 4.1 mini": {
        "ai_languageModel": [
          [
            {
              "node": "Model Selector",
              "type": "ai_languageModel",
              "index": 2
            }
          ]
        ]
      },
      "Request Type": {
        "main": [
          [
            {
              "node": "AI Agent",
              "type": "main",
              "index": 0
            }
          ]
        ]
      },
      "Simple Memory": {
        "ai_memory": [
          [
            {
              "node": "AI Agent",
              "type": "ai_memory",
              "index": 0
            }
          ]
        ]
      },
      "Model Selector": {
        "ai_languageModel": [
          [
            {
              "node": "AI Agent",
              "type": "ai_languageModel",
              "index": 0
            }
          ]
        ]
      },
      "OpenAI Chat Model": {
        "ai_languageModel": [
          [
            {
              "node": "Request Type",
              "type": "ai_languageModel",
              "index": 0
            }
          ]
        ]
      },
      "Gemini Thinking Pro": {
        "ai_languageModel": [
          [
            {
              "node": "Model Selector",
              "type": "ai_languageModel",
              "index": 1
            }
          ]
        ]
      },
      "Structured Output Parser": {
        "ai_outputParser": [
          [
            {
              "node": "Request Type",
              "type": "ai_outputParser",
              "index": 0
            }
          ]
        ]
      },
      "When chat message received": {
        "main": [
          [
            {
              "node": "Request Type",
              "type": "main",
              "index": 0
            }
          ]
        ]
      }
    }
  },
  "lastUpdatedBy": 29,
  "workflowInfo": {
    "nodeCount": 12,
    "nodeTypes": {
      "n8n-nodes-base.stickyNote": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.agent": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.chainLlm": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.chatTrigger": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.lmChatOpenAi": {
        "count": 2
      },
      "@n8n/n8n-nodes-langchain.modelSelector": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.lmChatAnthropic": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.lmChatOpenRouter": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.lmChatGoogleGemini": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.memoryBufferWindow": {
        "count": 1
      },
      "@n8n/n8n-nodes-langchain.outputParserStructured": {
        "count": 1
      }
    }
  },
  "user": {
    "name": "Davide",
    "username": "n3witalia",
    "bio": "Full-stack Web Developer based in Italy specialising in Marketing & AI-powered automations. For business enquiries, send me an email at info@n3w.it or add me on Linkedin.com/in/davideboizza",
    "verified": true,
    "links": [
      "https://n3w.it"
    ],
    "avatar": "https://gravatar.com/avatar/d41b8a0aa81139243509c58870f5b4be292824a507ab57d10ed066d8628ed8da?r=pg&d=retro&size=200"
  },
  "nodes": [
    {
      "id": 565,
      "icon": "fa:sticky-note",
      "name": "n8n-nodes-base.stickyNote",
      "codex": {
        "data": {
          "alias": [
            "Comments",
            "Notes",
            "Sticky"
          ],
          "categories": [
            "Core Nodes"
          ],
          "nodeVersion": "1.0",
          "codexVersion": "1.0",
          "subcategories": {
            "Core Nodes": [
              "Helpers"
            ]
          }
        }
      },
      "group": "[\"input\"]",
      "defaults": {
        "name": "Note collante",
        "color": "#FFD233"
      },
      "iconData": {
        "icon": "sticky-note",
        "type": "icon"
      },
      "displayName": "Sticky Note",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 9,
          "name": "Core Nodes"
        }
      ]
    },
    {
      "id": 1119,
      "icon": "fa:robot",
      "name": "@n8n/n8n-nodes-langchain.agent",
      "codex": {
        "data": {
          "alias": [
            "LangChain",
            "Chat",
            "Conversational",
            "Plan and Execute",
            "ReAct",
            "Tools"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Agents",
              "Root Nodes"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Agent IA",
        "color": "#404040"
      },
      "iconData": {
        "icon": "robot",
        "type": "icon"
      },
      "displayName": "AI Agent",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1123,
      "icon": "fa:link",
      "name": "@n8n/n8n-nodes-langchain.chainLlm",
      "codex": {
        "data": {
          "alias": [
            "LangChain"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Chains",
              "Root Nodes"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Chaîne LLM de base",
        "color": "#909298"
      },
      "iconData": {
        "icon": "link",
        "type": "icon"
      },
      "displayName": "Basic LLM Chain",
      "typeVersion": 2,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1145,
      "icon": "file:anthropic.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatAnthropic",
      "codex": {
        "data": {
          "alias": [
            "claude",
            "sonnet",
            "opus"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Modèle de chat Anthropic"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI0NiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iIzdEN0Q4NyIgZD0iTTMyLjczIDBoLTYuOTQ1TDM4LjQ1IDMyaDYuOTQ1ek0xMi42NjUgMCAwIDMyaDcuMDgybDIuNTktNi43MmgxMy4yNWwyLjU5IDYuNzJoNy4wODJMMTkuOTI5IDB6bS0uNzAyIDE5LjMzNyA0LjMzNC0xMS4yNDYgNC4zMzQgMTEuMjQ2eiIvPjwvc3ZnPg=="
      },
      "displayName": "Anthropic Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1153,
      "icon": "file:openAiLight.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Modèle de chat OpenAI"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,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"
      },
      "displayName": "OpenAI Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1163,
      "icon": "fa:database",
      "name": "@n8n/n8n-nodes-langchain.memoryBufferWindow",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Memory"
            ],
            "Memory": [
              "For beginners"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Mémoire simple"
      },
      "iconData": {
        "icon": "database",
        "type": "icon"
      },
      "displayName": "Simple Memory",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1179,
      "icon": "fa:code",
      "name": "@n8n/n8n-nodes-langchain.outputParserStructured",
      "codex": {
        "data": {
          "alias": [
            "json",
            "zod"
          ],
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Output Parsers"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Analyseur de sortie structuré"
      },
      "iconData": {
        "icon": "code",
        "type": "icon"
      },
      "displayName": "Structured Output Parser",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1247,
      "icon": "fa:comments",
      "name": "@n8n/n8n-nodes-langchain.chatTrigger",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.chattrigger/"
              }
            ]
          },
          "categories": [
            "Core Nodes",
            "Langchain"
          ]
        }
      },
      "group": "[\"trigger\"]",
      "defaults": {
        "name": "Lorsque le message de chat est reçu"
      },
      "iconData": {
        "icon": "comments",
        "type": "icon"
      },
      "displayName": "Chat Trigger",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 9,
          "name": "Core Nodes"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1262,
      "icon": "file:google.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatGoogleGemini",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Modèle de chat Google Gemini"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDggNDgiPjxkZWZzPjxwYXRoIGlkPSJhIiBkPSJNNDQuNSAyMEgyNHY4LjVoMTEuOEMzNC43IDMzLjkgMzAuMSAzNyAyNCAzN2MtNy4yIDAtMTMtNS44LTEzLTEzczUuOC0xMyAxMy0xM2MzLjEgMCA1LjkgMS4xIDguMSAyLjlsNi40LTYuNEMzNC42IDQuMSAyOS42IDIgMjQgMiAxMS44IDIgMiAxMS44IDIgMjRzOS44IDIyIDIyIDIyYzExIDAgMjEtOCAyMS0yMiAwLTEuMy0uMi0yLjctLjUtNCIvPjwvZGVmcz48Y2xpcFBhdGggaWQ9ImIiPjx1c2UgeGxpbms6aHJlZj0iI2EiIG92ZXJmbG93PSJ2aXNpYmxlIi8+PC9jbGlwUGF0aD48cGF0aCBmaWxsPSIjRkJCQzA1IiBkPSJNMCAzN1YxMWwxNyAxM3oiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiNFQTQzMzUiIGQ9Im0wIDExIDE3IDEzIDctNi4xTDQ4IDE0VjBIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiMzNEE4NTMiIGQ9Im0wIDM3IDMwLTIzIDcuOSAxTDQ4IDB2NDhIMHoiIGNsaXAtcGF0aD0idXJsKCNiKSIvPjxwYXRoIGZpbGw9IiM0Mjg1RjQiIGQ9Ik00OCA0OCAxNyAyNGwtNC0zIDM1LTEweiIgY2xpcC1wYXRoPSJ1cmwoI2IpIi8+PC9zdmc+"
      },
      "displayName": "Google Gemini Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1281,
      "icon": "file:openrouter.svg",
      "name": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenrouter/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models",
              "Root Nodes"
            ],
            "Language Models": [
              "Chat Models (Recommended)"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Modèle de chat OpenRouter"
      },
      "iconData": {
        "type": "file",
        "fileBuffer": "data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjOTRBM0I4IiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIHdpZHRoPSI0MCIgaGVpZ2h0PSI0MCIgdmlld0JveD0iMCAwIDI0IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjx0aXRsZT5PcGVuUm91dGVyPC90aXRsZT48cGF0aCBkPSJNMTYuODA0IDEuOTU3bDcuMjIgNC4xMDV2LjA4N0wxNi43MyAxMC4yMWwuMDE3LTIuMTE3LS44MjEtLjAzYy0xLjA1OS0uMDI4LTEuNjExLjAwMi0yLjI2OC4xMS0xLjA2NC4xNzUtMi4wMzguNTc3LTMuMTQ3IDEuMzUyTDguMzQ1IDExLjAzYy0uMjg0LjE5NS0uNDk1LjMzNi0uNjguNDU1bC0uNTE1LjMyMi0uMzk3LjIzNC4zODUuMjMuNTMuMzM4Yy40NzYuMzE0IDEuMTcuNzk2IDIuNzAxIDEuODY2IDEuMTEuNzc1IDIuMDgzIDEuMTc3IDMuMTQ3IDEuMzUybC4zLjA0NWMuNjk0LjA5MSAxLjM3NS4wOTQgMi44MjUuMDMzbC4wMjItMi4xNTkgNy4yMiA0LjEwNXYuMDg3TDE2LjU4OSAyMmwuMDE0LTEuODYyLS42MzUuMDIyYy0xLjM4Ni4wNDItMi4xMzcuMDAyLTMuMTM4LS4xNjItMS42OTQtLjI4LTMuMjYtLjkyNi00Ljg4MS0yLjA1OWwtMi4xNTgtMS41YTIxLjk5NyAyMS45OTcgMCAwMC0uNzU1LS40OThsLS40NjctLjI4YTU1LjkyNyA1NS45MjcgMCAwMC0uNzYtLjQzQzIuOTA4IDE0LjczLjU2MyAxNC4xMTYgMCAxNC4xMTZWOS44ODhsLjE0LjAwNGMuNTY0LS4wMDcgMi45MS0uNjIyIDMuODA5LTEuMTI0bDEuMDE2LS41OC40MzgtLjI3NGMuNDI4LS4yOCAxLjA3Mi0uNzI2IDIuNjg2LTEuODUzIDEuNjIxLTEuMTMzIDMuMTg2LTEuNzggNC44ODEtMi4wNTkgMS4xNTItLjE5IDEuOTc0LS4yMTMgMy44MTQtLjEzOGwuMDItMS45MDd6Ij48L3BhdGg+PC9zdmc+Cg=="
      },
      "displayName": "OpenRouter Chat Model",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    },
    {
      "id": 1306,
      "icon": "fa:map-signs",
      "name": "@n8n/n8n-nodes-langchain.modelSelector",
      "codex": {
        "data": {
          "resources": {
            "primaryDocumentation": [
              {
                "url": "https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.modelselector/"
              }
            ]
          },
          "categories": [
            "AI",
            "Langchain"
          ],
          "subcategories": {
            "AI": [
              "Language Models"
            ]
          }
        }
      },
      "group": "[\"transform\"]",
      "defaults": {
        "name": "Sélecteur de modèle"
      },
      "iconData": {
        "icon": "map-signs",
        "type": "icon"
      },
      "displayName": "Model Selector",
      "typeVersion": 1,
      "nodeCategories": [
        {
          "id": 25,
          "name": "AI"
        },
        {
          "id": 26,
          "name": "Langchain"
        }
      ]
    }
  ],
  "categories": [
    {
      "id": 5,
      "name": "Ingénierie"
    },
    {
      "id": 47,
      "name": "Chatbot IA"
    }
  ],
  "image": []
}